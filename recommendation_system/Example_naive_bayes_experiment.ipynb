{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD6DHQ7mVxpO"
      },
      "source": [
        "### Example Naive bayes in text processing on Sastrawi and Nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6yZ76vjVuxg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfF-R8kAWrxu"
      },
      "outputs": [],
      "source": [
        "products = pd.read_csv('../handle_sentiment/dataset/products_terbaruu.csv')\n",
        "reviews = pd.read_csv('../handle_sentiment/dataset/reviews_.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA2DsC8iXGAe"
      },
      "outputs": [],
      "source": [
        "products = products[['id', 'name', 'price', 'rating', 'slug_product', 'image_product']]\n",
        "reviews = reviews[['product', 'name','comment', 'rating', 'sentiment', 'user']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lejWUixh4sd"
      },
      "outputs": [],
      "source": [
        "reviews['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJJ5BRvyhjad"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "reviews['sentiment'].value_counts().plot(kind='bar')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Sentiment Rating Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNpolaKhork"
      },
      "source": [
        "### word cloud positif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1h5dnQghoE-"
      },
      "outputs": [],
      "source": [
        "# wordcloud review\n",
        "data_pos = reviews[reviews['sentiment'] == 'positif']\n",
        "\n",
        "all_text = ' '.join(word for word in data_pos['comment'])\n",
        "wordcloud = WordCloud(colormap='Greens', width=1000, height=800, mode='RGBA', background_color='white').generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(20,10), dpi=80)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCPosa8RjJYi"
      },
      "outputs": [],
      "source": [
        "data_neg = reviews[reviews['sentiment'] == 'negatif']\n",
        "\n",
        "all_text = ' '.join(word for word in data_neg['comment'])\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=800, mode='RGBA', background_color='white').generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(20,10), dpi=80)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQfnCNJmT8n2"
      },
      "source": [
        "### Teks Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfTJgov0T7Bj"
      },
      "outputs": [],
      "source": [
        "# cleaning text\n",
        "def cleaning_text(text):\n",
        "    # remove url\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text =  url_pattern.sub(r'', text)\n",
        "\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    text = re.sub(r'#', '', text)\n",
        "\n",
        "    # remove mention handle user (@)\n",
        "    text = re.sub(r'@[\\w]*', ' ', text)\n",
        "\n",
        "    # remove punctuation\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    for x in text.lower():\n",
        "        if x in punctuations:\n",
        "            text = text.replace(x, \" \")\n",
        "\n",
        "    # remove extra whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6qGd0MiShKi"
      },
      "source": [
        "### Custom Stopword List for Indonesian Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEyYchxdR5uT"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "rama_stopword = \"https://raw.githubusercontent.com/ramaprakoso/analisis-sentimen/master/kamus/stopword.txt\"\n",
        "yutomo_stopword = \"https://raw.githubusercontent.com/yasirutomo/python-sentianalysis-id/master/data/feature_list/stopwordsID.txt\"\n",
        "fpmipa_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/fpmipa-stopwords.txt\"\n",
        "sastrawi_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/sastrawi-stopwords.txt\"\n",
        "aliakbar_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/aliakbars-bilp.txt\"\n",
        "pebahasa_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/pebbie-pebahasa.txt\"\n",
        "elang_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-id.txt\"\n",
        "nltk_stopword = stopwords.words('indonesian')\n",
        "\n",
        "path_stopwords = [rama_stopword, yutomo_stopword, fpmipa_stopword, sastrawi_stopword,\n",
        "                  aliakbar_stopword, pebahasa_stopword, elang_stopword]\n",
        "\n",
        "# combine stopwords\n",
        "stopwords_l = nltk_stopword\n",
        "for path in path_stopwords:\n",
        "    response = requests.get(path)\n",
        "    stopwords_l += response.text.split('\\n')\n",
        "\n",
        "custom_st = '''\n",
        "Bagus sekali, Murah sedang ada promo. Barang OK, sudah dipakai lancar jaya, pengiriman cepat, semoga awet. Terima kasih seler terima kasih kurir.\n",
        "'''\n",
        "\n",
        "# create dictionary with unique stopword\n",
        "st_words = set(stopwords_l)\n",
        "custom_stopword = set(custom_st.split())\n",
        "\n",
        "# result stopwords\n",
        "stop_words = st_words | custom_stopword\n",
        "print(f'Stopwords: {list(stop_words)[:5]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3vhppwFTMMi"
      },
      "source": [
        "### remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sx3-U0PTLsF"
      },
      "outputs": [],
      "source": [
        "def remove_stopword(text, stop_words=stop_words):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    return ' '.join(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg_wYZiTTSaL"
      },
      "outputs": [],
      "source": [
        "def stemming_and_lemmatization(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    return stemmer.stem(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWHvurMKTc2p"
      },
      "source": [
        "### tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTs4tClcTYR6"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN_-RiOXTiP1"
      },
      "source": [
        "### Example Text Preprocessing Pipeline for Indonesian Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_9u5MBTiBm"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "text = 'Produk oke sesuai deskripsi dan normal berfungsi, pengiriman rapih dan buble wrap, dicoba langsung oke, respond sangat cepat, maaf baru beri nilai, belum sempat coba.. Recommend.. Dan langganan ahhh, sukses seller n shoope.'\n",
        "print(f'Original text: \\n{text}\\n')\n",
        "\n",
        "# cleaning text and lowercase\n",
        "text = cleaning_text(text)\n",
        "print(f'Cleaned text: \\n{text}\\n')\n",
        "\n",
        "# remove stopwords\n",
        "text = remove_stopword(text)\n",
        "print(f'Removed stopword: \\n{text}\\n')\n",
        "\n",
        "# stemming and lemmatization\n",
        "text = stemming_and_lemmatization(text)\n",
        "print(f'Stemmed and lemmatized: \\n{text}\\n')\n",
        "\n",
        "# tokenization\n",
        "text = tokenize(text)\n",
        "print(f'Tokenized: \\n{text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8utZaKLtUgVR"
      },
      "source": [
        "### pipeline preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqFzHwi5Ue6p"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_indonesia(text):\n",
        "    # cleaning text and lowercase\n",
        "    output = cleaning_text(text)\n",
        "\n",
        "    # remove stopwords\n",
        "    output = remove_stopword(output)\n",
        "\n",
        "    # stemming and lemmatization\n",
        "    output = stemming_and_lemmatization(output)\n",
        "\n",
        "    # tokenization\n",
        "    output = tokenize(output)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdffIplQUlvp"
      },
      "source": [
        "### test pipeline preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68mWcAsmUktR"
      },
      "outputs": [],
      "source": [
        "text = 'Produk oke sesuai deskripsi dan normal berfungsi, pengiriman rapih dan buble wrap, dicoba langsung oke, respond sangat cepat, maaf baru beri nilai, belum sempat coba.. Recommend.. Dan langganan ahhh, sukses seller n shoope.'\n",
        "text_preprocessing_indonesia(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsJSh4C1XVXQ"
      },
      "source": [
        "### Preprocessing Text Data in a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQv4xK5VXEbU"
      },
      "outputs": [],
      "source": [
        "reviews['comment'] = reviews['comment'].apply(text_preprocessing_indonesia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUZ30hdjUsLI"
      },
      "source": [
        "### Balancing the Dataset for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl4bzb-jkkvo"
      },
      "outputs": [],
      "source": [
        "negative_reviews = reviews[reviews['sentiment'] == 'negatif'].sample(n=min(620, len(reviews[reviews['sentiment'] == 'negatif'])), replace=True)\n",
        "positive_reviews = reviews[reviews['sentiment'] == 'positif'].sample(n=min(620, len(reviews[reviews['sentiment'] == 'positif'])), replace=True)\n",
        "sampled_reviews = pd.concat([negative_reviews, positive_reviews])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZUfH9h-U4Nx"
      },
      "source": [
        "### Encoding Sentiment Labels for Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyoCD_mPkoFu"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "sampled_reviews['sentiment_encoded'] = encoder.fit_transform(sampled_reviews['sentiment'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PwWxk17VBGa"
      },
      "source": [
        "### Splitting the Dataset for Model Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erhXALiSkqzt"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sampled_reviews['comment'], sampled_reviews['sentiment_encoded'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QQNVgOEZxYa"
      },
      "outputs": [],
      "source": [
        "X_train = [' '.join(tokens) for tokens in X_train]\n",
        "X_test = [' '.join(tokens) for tokens in X_test]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAHu9t_SVNWB"
      },
      "source": [
        "### Training a Multinomial Naive Bayes Model for Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OLqYDGykwl1"
      },
      "outputs": [],
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrm8fOVVTuT"
      },
      "source": [
        "### Making Predictions with the Trained Text Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB0mdrUpkx7I"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaGB5IkoVaEq"
      },
      "source": [
        "### Evaluating the Model with a Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI_CF8wvm1Mu"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INUFrkWyVfnK"
      },
      "source": [
        "### Evaluating Sentiment Analysis Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu1id5k7k1z9"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, target_names=encoder.classes_)\n",
        "\n",
        "print(\"Sentiment Analysis Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menghitung metrik evaluasi\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Membuat tabel evaluasi\n",
        "evaluation_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "evaluation_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l4D4gHUk27F"
      },
      "outputs": [],
      "source": [
        "test_reviews = [\"Produk ini sangat buruk! Saya sangat puas dengan kualitasnya.\",\n",
        "                \"Barangnya datang sangat buruk, dan pelayanannya baik.\",\n",
        "                \"Harganya sangat terjangkau mengingat kualitas yang ditawarkan\"]\n",
        "\n",
        "predicted_sentiments = []\n",
        "\n",
        "for review in test_reviews:\n",
        "    preprocessed_review = text_preprocessing_indonesia(review)\n",
        "    preprocessed_review_text = ' '.join(preprocessed_review)  # Convert the list of tokens into a single string\n",
        "    test_review_tfidf = tfidf_vectorizer.transform([preprocessed_review_text])\n",
        "    predicted_sentiment = model.predict(test_review_tfidf)\n",
        "    decoded_sentiment = encoder.inverse_transform(predicted_sentiment)\n",
        "    predicted_sentiments.append(decoded_sentiment[0])\n",
        "\n",
        "for i in range(len(test_reviews)):\n",
        "    print(f\"Komentar: {test_reviews[i]}\")\n",
        "    print(f\"Prediksi Sentimen: {predicted_sentiments[i]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FcEl_Q00vUO"
      },
      "source": [
        "### Merge the 'reviews' and 'products' dataframes based on 'product'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03mBlCWJ0wHA"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(reviews, products, left_on='product', right_on='id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVdineI0zYy"
      },
      "source": [
        "### Calculate the average rating for each product based on reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx6sVE2T0xxx"
      },
      "outputs": [],
      "source": [
        "\n",
        "average_review_ratings = merged_df.groupby('id')['rating_x'].mean().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBrO4Qf003PB"
      },
      "source": [
        "### Merge the average review ratings back into the 'products' dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBKNRo7Q01yK"
      },
      "outputs": [],
      "source": [
        "products['average_review_rating'] = average_review_ratings['rating_x']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQzEwHrx051Z"
      },
      "source": [
        "### Calculate the final combined rating as the average of product rating and average review rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaL1fAZ0041_"
      },
      "outputs": [],
      "source": [
        "products['combined_rating'] = (products['rating'] + products['average_review_rating']) / 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Ko87di09uf"
      },
      "source": [
        "### Sort products by highest predicted rating and lowest price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLWBZboW0_uF"
      },
      "outputs": [],
      "source": [
        "recommended_products = products.sort_values(by=['combined_rating', 'price'], ascending=[False, True])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubzo9wRPOHok"
      },
      "source": [
        "### Delete Average_review_ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCOCbx_FOHok"
      },
      "outputs": [],
      "source": [
        "recommended_products = recommended_products.drop('average_review_rating', axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnbavzbkh8wR"
      },
      "outputs": [],
      "source": [
        "recommended_products = recommended_products.drop('rating', axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GNss-hfgGD5"
      },
      "source": [
        "### Rename Rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQhtEKYogAAX"
      },
      "outputs": [],
      "source": [
        "recommended_products = recommended_products.rename(columns={'combined_rating': 'rating'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLiNGYAdOHok"
      },
      "source": [
        "### Print Recommended_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So0KBwQ51Cbo"
      },
      "outputs": [],
      "source": [
        "recommended_products[['name', 'price', 'image_product','rating']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POvb3FlMOHol"
      },
      "source": [
        "#### Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llNeXYygOHol"
      },
      "outputs": [],
      "source": [
        "output_file = 'recommended_products.pkl'\n",
        "\n",
        "# Simpan DataFrame ke dalam file pickle\n",
        "with open(output_file, 'wb') as file:\n",
        "    pickle.dump(recommended_products, file)\n",
        "\n",
        "print(f'Recommended products telah disimpan dalam file pickle: {output_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjpNTvnPOHom"
      },
      "outputs": [],
      "source": [
        "with open(output_file, 'rb') as file:\n",
        "    loaded_recommended_products = pickle.load(file)\n",
        "\n",
        "\n",
        "\n",
        "loaded_recommended_products"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
